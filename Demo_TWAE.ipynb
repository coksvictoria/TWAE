{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jxTFfNIgKsk",
        "outputId": "a1a79289-59e9-4818-a279-a68c3b3e8cee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt --quiet"
      ],
      "metadata": {
        "id": "IJVdLEfA1zuf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea5043e-1a41-4bd8-afcd-5fd68bf5d78e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m61.4/77.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.1/137.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.8/74.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.7/170.7 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ml_collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python s1_download_dataset.py\n",
        "!python s2_process_dataset.py"
      ],
      "metadata": {
        "id": "hPJxKmTXZc0U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bf10d94-0480-49ca-b876-cc0c2de53fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start processing dataset adult from UCI.\n",
            "Finish downloading dataset from https://archive.ics.uci.edu/static/public/2/adult.zip, data has been saved to data/adult.\n",
            "Finish unzipping adult.\n",
            "Start processing dataset abalone from UCI.\n",
            "Finish downloading dataset from https://archive.ics.uci.edu/static/public/1/abalone.zip, data has been saved to data/abalone.\n",
            "Finish unzipping abalone.\n",
            "dataset name : adult train size: (32561, 15), test size: (16281, 15)\n",
            "Numerical (32561, 6)\n",
            "Categorical (32561, 8)\n",
            "Processing and Saving adult Successfully!\n",
            "adult\n",
            "--------------------\n",
            "Total 48842\n",
            "Train 32561\n",
            "Test 16281\n",
            "N_Num 6\n",
            "N_Cat 9\n",
            "--------------------------------------------------\n",
            "dataset name : abalone train size: (3340, 9), test size: (836, 9)\n",
            "Numerical (3340, 7)\n",
            "Categorical (3340, 1)\n",
            "Processing and Saving abalone Successfully!\n",
            "abalone\n",
            "--------------------\n",
            "Total 4176\n",
            "Train 3340\n",
            "Test 836\n",
            "N_Num 8\n",
            "N_Cat 1\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --dataname 'adult' --method 'smote' --mode train\n",
        "!python main.py --dataname 'adult' --method 'synthpop' --mode train\n",
        "!python main.py --dataname 'adult' --method 'copula' --mode train"
      ],
      "metadata": {
        "id": "HdqDythBjocR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dcb1ed5-463c-4c90-cc46-de7d2f6a36fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------SMOTE----------------\n",
            "(32561, 15)\n",
            "Time:  1.7438397407531738\n",
            "Saving sampled data to synthetic/adult/SMOTE.csv\n",
            "-----------ADASYN----------------\n",
            "(32561, 15)\n",
            "Time:  2.6363437175750732\n",
            "Saving sampled data to synthetic/adult/ADASYN.csv\n",
            "-----------SMOTETomek----------------\n",
            "(32561, 15)\n",
            "Time:  4.5380706787109375\n",
            "Saving sampled data to synthetic/adult/SMOTETomek.csv\n",
            "train_age\n",
            "train_workclass\n",
            "train_fnlwgt\n",
            "train_education\n",
            "train_education.num\n",
            "train_marital.status\n",
            "train_occupation\n",
            "train_relationship\n",
            "train_race\n",
            "train_sex\n",
            "train_capital.gain\n",
            "train_capital.loss\n",
            "train_hours.per.week\n",
            "train_native.country\n",
            "train_income\n",
            "generate_age\n",
            "generate_workclass\n",
            "generate_fnlwgt\n",
            "generate_education\n",
            "generate_education.num\n",
            "generate_marital.status\n",
            "generate_occupation\n",
            "generate_relationship\n",
            "generate_race\n",
            "generate_sex\n",
            "generate_capital.gain\n",
            "generate_capital.loss\n",
            "generate_hours.per.week\n",
            "generate_native.country\n",
            "generate_income\n",
            "Time:  3.9691593647003174\n",
            "Saving sampled data to synthetic/adult/synthpop.csv\n",
            "Time:  12.067814350128174\n",
            "Saving sampled data to synthetic/adult/copula.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --dataname 'adult' --method 'ctgan' --mode train --sdv_epochs 200\n",
        "!python main.py --dataname 'adult' --method 'tvae' --mode train --sdv_epochs 200\n",
        "!python main.py --dataname 'adult' --method 'copulagan' --mode train --sdv_epochs 200"
      ],
      "metadata": {
        "id": "mZ0CZwtqcQ7t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "061546ac-069c-40ff-af5a-992af2c4ece5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gen. (-0.68) | Discrim. (-0.02): 100% 201/201 [06:01<00:00,  1.80s/it]\n",
            "Time:  391.99858236312866\n",
            "Saving sampled data to synthetic/adult/ctgan.csv\n",
            "Time:  171.51639127731323\n",
            "Saving sampled data to synthetic/adult/tvae.csv\n",
            "Time:  393.76494002342224\n",
            "Saving sampled data to synthetic/adult/copulagan.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --dataname 'adult' --method 'ctabgan' --mode train --sdv_epochs 300\n",
        "!python main.py --dataname 'adult' --method 'ctabgan' --mode sample"
      ],
      "metadata": {
        "id": "LL28YTR-cQUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c61e5944-040b-4454-ecdb-24277bde0825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['age', 'workclass', 'fnlwgt', 'education', 'education.num', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'capital.gain', 'capital.loss', 'hours.per.week', 'native.country', 'income']\n",
            "['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country', 'income']\n",
            "/content/drive/MyDrive/TDS/baselines/ctabgan/ckpt/adult\n",
            "100% 300/300 [1:03:04<00:00, 12.61s/it]\n",
            "Finished training in 3811.9643094539642  seconds.\n",
            "Time: 3.0813755989074707\n",
            "Saving sampled data to synthetic/adult/ctabgan.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --dataname 'adult' --method 'tabddpm' --mode train --tabddpm_epochs 1000\n",
        "!python main.py --dataname 'adult' --method 'tabddpm' --mode sample"
      ],
      "metadata": {
        "id": "D0dhdxdf4MT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bd649d9-5524-4c6f-a307-658a3e510adc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "START TRAINING\n",
            "No NaNs in numerical features, skipping\n",
            "110\n",
            "{'num_classes': 2, 'is_y_cond': False, 'rtdl_params': {'d_layers': [1024, 2048, 2048, 1024], 'dropout': 0.0}, 'd_in': 110}\n",
            "mlp\n",
            "MLPDiffusion(\n",
            "  (mlp): MLP(\n",
            "    (blocks): ModuleList(\n",
            "      (0): Block(\n",
            "        (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (1): Block(\n",
            "        (linear): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "        (activation): ReLU()\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (2): Block(\n",
            "        (linear): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "        (activation): ReLU()\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (3): Block(\n",
            "        (linear): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (head): Linear(in_features=1024, out_features=110, bias=True)\n",
            "  )\n",
            "  (proj): Linear(in_features=110, out_features=1024, bias=True)\n",
            "  (time_embed): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (1): SiLU()\n",
            "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            ")\n",
            "the number of parameters 11768942\n",
            "Steps:  1000\n",
            "Step 100/1000 MLoss: 2.0519 GLoss: 1597.0173 Sum: 1599.0692\n",
            "Step 200/1000 MLoss: 1.1585 GLoss: 1.0031 Sum: 2.1616\n",
            "Step 300/1000 MLoss: 1.1303 GLoss: 1.0009 Sum: 2.1312\n",
            "Step 400/1000 MLoss: 1.1279 GLoss: 0.9982 Sum: 2.1261\n",
            "Step 500/1000 MLoss: 1.1267 GLoss: 0.9987 Sum: 2.1254\n",
            "Step 600/1000 MLoss: 1.1198 GLoss: 0.999 Sum: 2.1188\n",
            "Step 700/1000 MLoss: 1.1152 GLoss: 1.0027 Sum: 2.1179\n",
            "Step 800/1000 MLoss: 1.1046 GLoss: 1.0006 Sum: 2.1052\n",
            "Step 900/1000 MLoss: 1.1055 GLoss: 1.0034 Sum: 2.1089\n",
            "Step 1000/1000 MLoss: 1.1103 GLoss: 1.0008 Sum: 2.1111\n",
            "synthetic/adult\n",
            "START SAMPLING\n",
            "No NaNs in numerical features, skipping\n",
            "mlp\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Shape torch.Size([32561, 15])\n",
            "(32561, 9)\n",
            "Sampling time: 240.98338150978088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --dataname 'adult' --method 'twae' --mode train --twae_epochs 300\n",
        "!python main.py --dataname 'adult' --method 'twae' --mode sample --lsi_method 'rectangle' #SMOTE or triangle"
      ],
      "metadata": {
        "id": "OwDaKLlnye-z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07f0466f-af7e-4ce9-d8e4-5f06aa941e3f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['age', 'workclass', 'fnlwgt', 'education', 'education.num', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'capital.gain', 'capital.loss', 'hours.per.week', 'native.country', 'income']\n",
            "['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country', 'income']\n",
            "/content/drive/MyDrive/TDS/twae/ckpt/adult\n",
            "Epoch 1/300: 100% 66/66 [00:03<00:00, 18.80it/s, Loss=21.7]\n",
            "Epoch 2/300: 100% 66/66 [00:01<00:00, 40.32it/s, Loss=13.2]\n",
            "Epoch 3/300: 100% 66/66 [00:01<00:00, 41.42it/s, Loss=5.13]\n",
            "Epoch 4/300: 100% 66/66 [00:01<00:00, 41.31it/s, Loss=0.317]\n",
            "Epoch 5/300: 100% 66/66 [00:01<00:00, 41.58it/s, Loss=-2.5]\n",
            "Epoch 6/300: 100% 66/66 [00:01<00:00, 41.61it/s, Loss=-3.13]\n",
            "Epoch 7/300: 100% 66/66 [00:01<00:00, 40.48it/s, Loss=-5.83]\n",
            "Epoch 8/300: 100% 66/66 [00:01<00:00, 37.13it/s, Loss=-2.88]\n",
            "Epoch 9/300: 100% 66/66 [00:01<00:00, 41.33it/s, Loss=-7.8]\n",
            "Epoch 10/300: 100% 66/66 [00:01<00:00, 41.21it/s, Loss=-9.79]\n",
            "Epoch 11/300: 100% 66/66 [00:01<00:00, 41.56it/s, Loss=-9.54]\n",
            "Epoch 12/300: 100% 66/66 [00:01<00:00, 41.55it/s, Loss=-8.26]\n",
            "Epoch 13/300: 100% 66/66 [00:01<00:00, 41.57it/s, Loss=-15.3]\n",
            "Epoch 14/300: 100% 66/66 [00:01<00:00, 41.22it/s, Loss=-10.8]\n",
            "Epoch 15/300: 100% 66/66 [00:01<00:00, 41.66it/s, Loss=-14.5]\n",
            "Epoch 16/300: 100% 66/66 [00:01<00:00, 41.23it/s, Loss=-17.9]\n",
            "Epoch 17/300: 100% 66/66 [00:01<00:00, 41.61it/s, Loss=-12.3]\n",
            "Epoch 18/300: 100% 66/66 [00:01<00:00, 41.91it/s, Loss=-5.29]\n",
            "Epoch 19/300: 100% 66/66 [00:01<00:00, 41.39it/s, Loss=-9.36]\n",
            "Epoch 20/300: 100% 66/66 [00:01<00:00, 41.39it/s, Loss=-17.5]\n",
            "Epoch 21/300: 100% 66/66 [00:01<00:00, 41.33it/s, Loss=-18.8]\n",
            "Epoch 22/300: 100% 66/66 [00:01<00:00, 38.01it/s, Loss=-19.9]\n",
            "Epoch 23/300: 100% 66/66 [00:01<00:00, 41.60it/s, Loss=-17.8]\n",
            "Epoch 24/300: 100% 66/66 [00:01<00:00, 40.72it/s, Loss=-21]\n",
            "Epoch 25/300: 100% 66/66 [00:01<00:00, 41.41it/s, Loss=-19.1]\n",
            "Epoch 26/300: 100% 66/66 [00:01<00:00, 41.54it/s, Loss=-20.3]\n",
            "Epoch 27/300: 100% 66/66 [00:01<00:00, 41.40it/s, Loss=-18.1]\n",
            "Epoch 28/300: 100% 66/66 [00:01<00:00, 41.14it/s, Loss=-23.7]\n",
            "Epoch 29/300: 100% 66/66 [00:01<00:00, 41.66it/s, Loss=-15.8]\n",
            "Epoch 30/300: 100% 66/66 [00:01<00:00, 41.41it/s, Loss=-20.9]\n",
            "Epoch 31/300: 100% 66/66 [00:01<00:00, 40.92it/s, Loss=-25.7]\n",
            "Epoch 32/300: 100% 66/66 [00:01<00:00, 40.90it/s, Loss=-18.5]\n",
            "Epoch 33/300: 100% 66/66 [00:01<00:00, 37.84it/s, Loss=-22.6]\n",
            "Epoch 34/300: 100% 66/66 [00:01<00:00, 41.64it/s, Loss=-23.5]\n",
            "Epoch 35/300: 100% 66/66 [00:01<00:00, 41.21it/s, Loss=-11.5]\n",
            "Epoch 36/300: 100% 66/66 [00:01<00:00, 41.47it/s, Loss=-25.3]\n",
            "Epoch 37/300: 100% 66/66 [00:01<00:00, 41.57it/s, Loss=-30.8]\n",
            "Epoch 38/300: 100% 66/66 [00:01<00:00, 41.47it/s, Loss=-26.8]\n",
            "Epoch 39/300: 100% 66/66 [00:01<00:00, 41.29it/s, Loss=-20.4]\n",
            "Epoch 40/300: 100% 66/66 [00:01<00:00, 37.24it/s, Loss=-23.6]\n",
            "Epoch 41/300: 100% 66/66 [00:01<00:00, 41.44it/s, Loss=-25.8]\n",
            "Epoch 42/300: 100% 66/66 [00:01<00:00, 41.64it/s, Loss=-24.6]\n",
            "Epoch 43/300: 100% 66/66 [00:01<00:00, 41.38it/s, Loss=-26.1]\n",
            "Epoch 44/300: 100% 66/66 [00:01<00:00, 41.46it/s, Loss=-29.3]\n",
            "Epoch 45/300: 100% 66/66 [00:01<00:00, 41.33it/s, Loss=-27.2]\n",
            "Epoch 46/300: 100% 66/66 [00:01<00:00, 41.65it/s, Loss=-25.5]\n",
            "Epoch 47/300: 100% 66/66 [00:01<00:00, 41.43it/s, Loss=-33.4]\n",
            "Epoch 48/300: 100% 66/66 [00:01<00:00, 37.59it/s, Loss=-28.6]\n",
            "Epoch 49/300: 100% 66/66 [00:01<00:00, 41.29it/s, Loss=-28.5]\n",
            "Epoch 50/300: 100% 66/66 [00:01<00:00, 41.44it/s, Loss=-22.3]\n",
            "Epoch 51/300: 100% 66/66 [00:01<00:00, 41.48it/s, Loss=-30.4]\n",
            "Epoch 52/300: 100% 66/66 [00:01<00:00, 41.65it/s, Loss=-30.6]\n",
            "Epoch 53/300: 100% 66/66 [00:01<00:00, 41.24it/s, Loss=-26.9]\n",
            "Epoch 54/300: 100% 66/66 [00:01<00:00, 41.56it/s, Loss=-32.4]\n",
            "Epoch 55/300: 100% 66/66 [00:01<00:00, 41.26it/s, Loss=-32.1]\n",
            "Epoch 56/300: 100% 66/66 [00:01<00:00, 40.44it/s, Loss=-26.9]\n",
            "Epoch 57/300: 100% 66/66 [00:01<00:00, 37.52it/s, Loss=-29]\n",
            "Epoch 58/300: 100% 66/66 [00:01<00:00, 41.29it/s, Loss=-32.6]\n",
            "Epoch 59/300: 100% 66/66 [00:01<00:00, 41.46it/s, Loss=-32.6]\n",
            "Epoch 60/300: 100% 66/66 [00:01<00:00, 41.65it/s, Loss=-33.8]\n",
            "Epoch 61/300: 100% 66/66 [00:01<00:00, 41.24it/s, Loss=-34.9]\n",
            "Epoch 62/300: 100% 66/66 [00:01<00:00, 41.31it/s, Loss=-33.4]\n",
            "Epoch 63/300: 100% 66/66 [00:01<00:00, 41.07it/s, Loss=-35.8]\n",
            "Epoch 64/300: 100% 66/66 [00:01<00:00, 41.59it/s, Loss=-33.5]\n",
            "Epoch 65/300: 100% 66/66 [00:01<00:00, 41.29it/s, Loss=-37.2]\n",
            "Epoch 66/300: 100% 66/66 [00:01<00:00, 41.48it/s, Loss=-31.7]\n",
            "Epoch 67/300: 100% 66/66 [00:01<00:00, 37.86it/s, Loss=-34.6]\n",
            "Epoch 68/300: 100% 66/66 [00:01<00:00, 41.45it/s, Loss=-34]\n",
            "Epoch 69/300: 100% 66/66 [00:01<00:00, 41.38it/s, Loss=-35.8]\n",
            "Epoch 70/300: 100% 66/66 [00:01<00:00, 41.71it/s, Loss=-33.5]\n",
            "Epoch 71/300: 100% 66/66 [00:01<00:00, 41.71it/s, Loss=-34.1]\n",
            "Epoch 72/300: 100% 66/66 [00:01<00:00, 41.34it/s, Loss=-31.9]\n",
            "Epoch 73/300: 100% 66/66 [00:01<00:00, 41.10it/s, Loss=-37]\n",
            "Epoch 74/300: 100% 66/66 [00:01<00:00, 38.02it/s, Loss=-35.5]\n",
            "Epoch 75/300: 100% 66/66 [00:01<00:00, 41.55it/s, Loss=-31.7]\n",
            "Epoch 76/300: 100% 66/66 [00:01<00:00, 41.45it/s, Loss=-35]\n",
            "Epoch 77/300: 100% 66/66 [00:01<00:00, 41.57it/s, Loss=-35.4]\n",
            "Epoch 78/300: 100% 66/66 [00:01<00:00, 41.11it/s, Loss=-24.4]\n",
            "Epoch 79/300: 100% 66/66 [00:01<00:00, 41.31it/s, Loss=-32.7]\n",
            "Epoch 80/300: 100% 66/66 [00:01<00:00, 40.65it/s, Loss=-34.2]\n",
            "Epoch 81/300: 100% 66/66 [00:01<00:00, 40.92it/s, Loss=-37.2]\n",
            "Epoch 82/300: 100% 66/66 [00:01<00:00, 41.31it/s, Loss=-32]\n",
            "Epoch 83/300: 100% 66/66 [00:01<00:00, 37.93it/s, Loss=-30.4]\n",
            "Epoch 84/300: 100% 66/66 [00:01<00:00, 41.28it/s, Loss=-35.6]\n",
            "Epoch 85/300: 100% 66/66 [00:01<00:00, 41.78it/s, Loss=-37.7]\n",
            "Epoch 86/300: 100% 66/66 [00:01<00:00, 41.64it/s, Loss=-38.5]\n",
            "Epoch 87/300: 100% 66/66 [00:01<00:00, 41.27it/s, Loss=-39]\n",
            "Epoch 88/300: 100% 66/66 [00:01<00:00, 41.12it/s, Loss=-38.7]\n",
            "Epoch 89/300: 100% 66/66 [00:01<00:00, 41.24it/s, Loss=-40.1]\n",
            "Epoch 90/300: 100% 66/66 [00:01<00:00, 41.38it/s, Loss=-39.7]\n",
            "Epoch 91/300: 100% 66/66 [00:01<00:00, 41.28it/s, Loss=-40.8]\n",
            "Epoch 92/300: 100% 66/66 [00:01<00:00, 37.99it/s, Loss=-39.6]\n",
            "Epoch 93/300: 100% 66/66 [00:01<00:00, 41.56it/s, Loss=-35.5]\n",
            "Epoch 94/300: 100% 66/66 [00:01<00:00, 41.56it/s, Loss=-29.1]\n",
            "Epoch 95/300: 100% 66/66 [00:01<00:00, 41.58it/s, Loss=nan]\n",
            "Epoch 96/300: 100% 66/66 [00:01<00:00, 41.21it/s, Loss=nan]\n",
            "Epoch 97/300: 100% 66/66 [00:01<00:00, 40.62it/s, Loss=nan]\n",
            "Epoch 98/300: 100% 66/66 [00:01<00:00, 40.65it/s, Loss=nan]\n",
            "Epoch 99/300: 100% 66/66 [00:01<00:00, 41.56it/s, Loss=nan]\n",
            "Epoch 100/300: 100% 66/66 [00:01<00:00, 41.73it/s, Loss=nan]\n",
            "Epoch 101/300: 100% 66/66 [00:01<00:00, 41.65it/s, Loss=nan]\n",
            "Epoch 102/300: 100% 66/66 [00:01<00:00, 41.64it/s, Loss=nan]\n",
            "Epoch 103/300: 100% 66/66 [00:01<00:00, 41.38it/s, Loss=nan]\n",
            "Epoch 104/300: 100% 66/66 [00:01<00:00, 41.55it/s, Loss=nan]\n",
            "Epoch 105/300: 100% 66/66 [00:01<00:00, 40.96it/s, Loss=nan]\n",
            "Epoch 106/300: 100% 66/66 [00:01<00:00, 37.40it/s, Loss=nan]\n",
            "Epoch 107/300: 100% 66/66 [00:01<00:00, 41.35it/s, Loss=nan]\n",
            "Epoch 108/300: 100% 66/66 [00:01<00:00, 41.55it/s, Loss=nan]\n",
            "Epoch 109/300: 100% 66/66 [00:01<00:00, 41.45it/s, Loss=nan]\n",
            "Epoch 110/300: 100% 66/66 [00:01<00:00, 41.46it/s, Loss=nan]\n",
            "Epoch 111/300: 100% 66/66 [00:01<00:00, 41.66it/s, Loss=nan]\n",
            "Epoch 00111: reducing learning rate of group 0 to 9.0000e-04.\n",
            "Epoch 112/300: 100% 66/66 [00:01<00:00, 41.40it/s, Loss=nan]\n",
            "Epoch 113/300: 100% 66/66 [00:01<00:00, 37.97it/s, Loss=nan]\n",
            "Epoch 114/300: 100% 66/66 [00:01<00:00, 41.20it/s, Loss=nan]\n",
            "Epoch 115/300: 100% 66/66 [00:01<00:00, 41.07it/s, Loss=nan]\n",
            "Epoch 116/300: 100% 66/66 [00:01<00:00, 41.54it/s, Loss=nan]\n",
            "Epoch 117/300: 100% 66/66 [00:01<00:00, 41.61it/s, Loss=nan]\n",
            "Epoch 118/300: 100% 66/66 [00:01<00:00, 41.49it/s, Loss=nan]\n",
            "Epoch 119/300: 100% 66/66 [00:01<00:00, 41.71it/s, Loss=nan]\n",
            "Epoch 120/300: 100% 66/66 [00:01<00:00, 41.81it/s, Loss=nan]\n",
            "Epoch 121/300: 100% 66/66 [00:01<00:00, 41.39it/s, Loss=nan]\n",
            "Epoch 122/300: 100% 66/66 [00:01<00:00, 41.29it/s, Loss=nan]\n",
            "Epoch 123/300: 100% 66/66 [00:01<00:00, 40.99it/s, Loss=nan]\n",
            "Epoch 124/300: 100% 66/66 [00:01<00:00, 41.26it/s, Loss=nan]\n",
            "Epoch 125/300: 100% 66/66 [00:01<00:00, 41.39it/s, Loss=nan]\n",
            "Epoch 126/300: 100% 66/66 [00:01<00:00, 37.85it/s, Loss=nan]\n",
            "Epoch 127/300: 100% 66/66 [00:01<00:00, 41.54it/s, Loss=nan]\n",
            "Epoch 128/300: 100% 66/66 [00:01<00:00, 41.47it/s, Loss=nan]\n",
            "Epoch 129/300: 100% 66/66 [00:01<00:00, 41.52it/s, Loss=nan]\n",
            "Epoch 130/300: 100% 66/66 [00:01<00:00, 41.38it/s, Loss=nan]\n",
            "Epoch 131/300: 100% 66/66 [00:01<00:00, 41.08it/s, Loss=nan]\n",
            "Epoch 132/300: 100% 66/66 [00:01<00:00, 40.83it/s, Loss=nan]\n",
            "Epoch 00132: reducing learning rate of group 0 to 8.1000e-04.\n",
            "Epoch 133/300: 100% 66/66 [00:01<00:00, 38.02it/s, Loss=nan]\n",
            "Epoch 134/300: 100% 66/66 [00:01<00:00, 41.57it/s, Loss=nan]\n",
            "Epoch 135/300: 100% 66/66 [00:01<00:00, 41.45it/s, Loss=nan]\n",
            "Epoch 136/300: 100% 66/66 [00:01<00:00, 41.38it/s, Loss=nan]\n",
            "Epoch 137/300: 100% 66/66 [00:01<00:00, 41.29it/s, Loss=nan]\n",
            "Epoch 138/300: 100% 66/66 [00:01<00:00, 41.17it/s, Loss=nan]\n",
            "Epoch 139/300: 100% 66/66 [00:01<00:00, 41.18it/s, Loss=nan]\n",
            "Epoch 140/300: 100% 66/66 [00:01<00:00, 40.93it/s, Loss=nan]\n",
            "Epoch 141/300: 100% 66/66 [00:01<00:00, 41.75it/s, Loss=nan]\n",
            "Epoch 142/300: 100% 66/66 [00:01<00:00, 41.75it/s, Loss=nan]\n",
            "Epoch 143/300: 100% 66/66 [00:01<00:00, 41.75it/s, Loss=nan]\n",
            "Epoch 144/300: 100% 66/66 [00:01<00:00, 41.95it/s, Loss=nan]\n",
            "Epoch 145/300: 100% 66/66 [00:01<00:00, 41.89it/s, Loss=nan]\n",
            "Epoch 146/300: 100% 66/66 [00:01<00:00, 38.07it/s, Loss=nan]\n",
            "Epoch 147/300: 100% 66/66 [00:01<00:00, 40.96it/s, Loss=nan]\n",
            "Epoch 148/300: 100% 66/66 [00:01<00:00, 41.08it/s, Loss=nan]\n",
            "Epoch 149/300: 100% 66/66 [00:01<00:00, 41.52it/s, Loss=nan]\n",
            "Epoch 150/300: 100% 66/66 [00:01<00:00, 41.27it/s, Loss=nan]\n",
            "Epoch 151/300: 100% 66/66 [00:01<00:00, 41.65it/s, Loss=nan]\n",
            "Epoch 152/300: 100% 66/66 [00:01<00:00, 41.82it/s, Loss=nan]\n",
            "Epoch 153/300: 100% 66/66 [00:01<00:00, 38.28it/s, Loss=nan]\n",
            "Epoch 00153: reducing learning rate of group 0 to 7.2900e-04.\n",
            "Epoch 154/300: 100% 66/66 [00:01<00:00, 41.55it/s, Loss=nan]\n",
            "Epoch 155/300: 100% 66/66 [00:01<00:00, 41.27it/s, Loss=nan]\n",
            "Epoch 156/300: 100% 66/66 [00:01<00:00, 41.21it/s, Loss=nan]\n",
            "Epoch 157/300: 100% 66/66 [00:01<00:00, 41.08it/s, Loss=nan]\n",
            "Epoch 158/300: 100% 66/66 [00:01<00:00, 41.47it/s, Loss=nan]\n",
            "Epoch 159/300: 100% 66/66 [00:01<00:00, 41.61it/s, Loss=nan]\n",
            "Epoch 160/300: 100% 66/66 [00:01<00:00, 41.62it/s, Loss=nan]\n",
            "Epoch 161/300: 100% 66/66 [00:01<00:00, 41.64it/s, Loss=nan]\n",
            "Epoch 162/300: 100% 66/66 [00:01<00:00, 41.51it/s, Loss=nan]\n",
            "Epoch 163/300: 100% 66/66 [00:01<00:00, 41.57it/s, Loss=nan]\n",
            "Epoch 164/300: 100% 66/66 [00:01<00:00, 41.17it/s, Loss=nan]\n",
            "Epoch 165/300: 100% 66/66 [00:01<00:00, 41.18it/s, Loss=nan]\n",
            "Epoch 166/300: 100% 66/66 [00:01<00:00, 41.47it/s, Loss=nan]\n",
            "Epoch 167/300: 100% 66/66 [00:01<00:00, 37.93it/s, Loss=nan]\n",
            "Epoch 168/300: 100% 66/66 [00:01<00:00, 41.50it/s, Loss=nan]\n",
            "Epoch 169/300: 100% 66/66 [00:01<00:00, 41.61it/s, Loss=nan]\n",
            "Epoch 170/300: 100% 66/66 [00:01<00:00, 41.69it/s, Loss=nan]\n",
            "Epoch 171/300: 100% 66/66 [00:01<00:00, 41.68it/s, Loss=nan]\n",
            "Epoch 172/300: 100% 66/66 [00:01<00:00, 41.31it/s, Loss=nan]\n",
            "Epoch 173/300: 100% 66/66 [00:01<00:00, 37.62it/s, Loss=nan]\n",
            "Epoch 174/300: 100% 66/66 [00:01<00:00, 40.96it/s, Loss=nan]\n",
            "Epoch 00174: reducing learning rate of group 0 to 6.5610e-04.\n",
            "Epoch 175/300: 100% 66/66 [00:01<00:00, 41.37it/s, Loss=nan]\n",
            "Epoch 176/300: 100% 66/66 [00:01<00:00, 41.47it/s, Loss=nan]\n",
            "Epoch 177/300: 100% 66/66 [00:01<00:00, 41.43it/s, Loss=nan]\n",
            "Epoch 178/300: 100% 66/66 [00:01<00:00, 41.09it/s, Loss=nan]\n",
            "Epoch 179/300: 100% 66/66 [00:01<00:00, 40.95it/s, Loss=nan]\n",
            "Epoch 180/300: 100% 66/66 [00:01<00:00, 40.86it/s, Loss=nan]\n",
            "Epoch 181/300: 100% 66/66 [00:01<00:00, 40.70it/s, Loss=nan]\n",
            "Epoch 182/300: 100% 66/66 [00:01<00:00, 41.22it/s, Loss=nan]\n",
            "Epoch 183/300: 100% 66/66 [00:01<00:00, 41.50it/s, Loss=nan]\n",
            "Epoch 184/300: 100% 66/66 [00:01<00:00, 41.60it/s, Loss=nan]\n",
            "Epoch 185/300: 100% 66/66 [00:01<00:00, 41.57it/s, Loss=nan]\n",
            "Epoch 186/300: 100% 66/66 [00:01<00:00, 41.55it/s, Loss=nan]\n",
            "Epoch 187/300: 100% 66/66 [00:01<00:00, 37.76it/s, Loss=nan]\n",
            "Epoch 188/300: 100% 66/66 [00:01<00:00, 41.78it/s, Loss=nan]\n",
            "Epoch 189/300: 100% 66/66 [00:01<00:00, 40.93it/s, Loss=nan]\n",
            "Epoch 190/300: 100% 66/66 [00:01<00:00, 40.80it/s, Loss=nan]\n",
            "Epoch 191/300: 100% 66/66 [00:01<00:00, 41.13it/s, Loss=nan]\n",
            "Epoch 192/300: 100% 66/66 [00:01<00:00, 41.67it/s, Loss=nan]\n",
            "Epoch 193/300: 100% 66/66 [00:01<00:00, 41.70it/s, Loss=nan]\n",
            "Epoch 194/300: 100% 66/66 [00:01<00:00, 38.09it/s, Loss=nan]\n",
            "Epoch 195/300: 100% 66/66 [00:01<00:00, 41.60it/s, Loss=nan]\n",
            "Epoch 00195: reducing learning rate of group 0 to 5.9049e-04.\n",
            "Epoch 196/300: 100% 66/66 [00:01<00:00, 41.55it/s, Loss=nan]\n",
            "Epoch 197/300: 100% 66/66 [00:01<00:00, 41.03it/s, Loss=nan]\n",
            "Epoch 198/300: 100% 66/66 [00:01<00:00, 40.92it/s, Loss=nan]\n",
            "Epoch 199/300: 100% 66/66 [00:01<00:00, 41.07it/s, Loss=nan]\n",
            "Epoch 200/300: 100% 66/66 [00:01<00:00, 41.59it/s, Loss=nan]\n",
            "Epoch 201/300: 100% 66/66 [00:01<00:00, 41.58it/s, Loss=nan]\n",
            "Epoch 202/300: 100% 66/66 [00:01<00:00, 41.66it/s, Loss=nan]\n",
            "Epoch 203/300: 100% 66/66 [00:01<00:00, 41.80it/s, Loss=nan]\n",
            "Epoch 204/300: 100% 66/66 [00:01<00:00, 41.47it/s, Loss=nan]\n",
            "Epoch 205/300: 100% 66/66 [00:01<00:00, 41.27it/s, Loss=nan]\n",
            "Epoch 206/300: 100% 66/66 [00:01<00:00, 40.78it/s, Loss=nan]\n",
            "Epoch 207/300: 100% 66/66 [00:01<00:00, 37.60it/s, Loss=nan]\n",
            "Epoch 208/300: 100% 66/66 [00:01<00:00, 41.52it/s, Loss=nan]\n",
            "Epoch 209/300: 100% 66/66 [00:01<00:00, 41.59it/s, Loss=nan]\n",
            "Epoch 210/300: 100% 66/66 [00:01<00:00, 41.67it/s, Loss=nan]\n",
            "Epoch 211/300: 100% 66/66 [00:01<00:00, 41.40it/s, Loss=nan]\n",
            "Epoch 212/300: 100% 66/66 [00:01<00:00, 41.46it/s, Loss=nan]\n",
            "Epoch 213/300: 100% 66/66 [00:01<00:00, 41.56it/s, Loss=nan]\n",
            "Epoch 214/300: 100% 66/66 [00:01<00:00, 37.45it/s, Loss=nan]\n",
            "Epoch 215/300: 100% 66/66 [00:01<00:00, 41.04it/s, Loss=nan]\n",
            "Epoch 216/300: 100% 66/66 [00:01<00:00, 41.65it/s, Loss=nan]\n",
            "Epoch 00216: reducing learning rate of group 0 to 5.3144e-04.\n",
            "Epoch 217/300: 100% 66/66 [00:01<00:00, 41.75it/s, Loss=nan]\n",
            "Epoch 218/300: 100% 66/66 [00:01<00:00, 41.55it/s, Loss=nan]\n",
            "Epoch 219/300: 100% 66/66 [00:01<00:00, 41.52it/s, Loss=nan]\n",
            "Epoch 220/300: 100% 66/66 [00:01<00:00, 41.69it/s, Loss=nan]\n",
            "Epoch 221/300: 100% 66/66 [00:01<00:00, 41.54it/s, Loss=nan]\n",
            "Epoch 222/300: 100% 66/66 [00:01<00:00, 41.28it/s, Loss=nan]\n",
            "Epoch 223/300: 100% 66/66 [00:01<00:00, 41.47it/s, Loss=nan]\n",
            "Epoch 224/300: 100% 66/66 [00:01<00:00, 41.51it/s, Loss=nan]\n",
            "Epoch 225/300: 100% 66/66 [00:01<00:00, 41.88it/s, Loss=nan]\n",
            "Epoch 226/300: 100% 66/66 [00:01<00:00, 41.68it/s, Loss=nan]\n",
            "Epoch 227/300: 100% 66/66 [00:01<00:00, 41.73it/s, Loss=nan]\n",
            "Epoch 228/300: 100% 66/66 [00:01<00:00, 38.26it/s, Loss=nan]\n",
            "Epoch 229/300: 100% 66/66 [00:01<00:00, 41.64it/s, Loss=nan]\n",
            "Epoch 230/300: 100% 66/66 [00:01<00:00, 41.28it/s, Loss=nan]\n",
            "Epoch 231/300: 100% 66/66 [00:01<00:00, 41.33it/s, Loss=nan]\n",
            "Epoch 232/300: 100% 66/66 [00:01<00:00, 41.73it/s, Loss=nan]\n",
            "Epoch 233/300: 100% 66/66 [00:01<00:00, 41.51it/s, Loss=nan]\n",
            "Epoch 234/300: 100% 66/66 [00:01<00:00, 38.00it/s, Loss=nan]\n",
            "Epoch 235/300: 100% 66/66 [00:01<00:00, 41.78it/s, Loss=nan]\n",
            "Epoch 236/300: 100% 66/66 [00:01<00:00, 41.90it/s, Loss=nan]\n",
            "Epoch 237/300: 100% 66/66 [00:01<00:00, 41.66it/s, Loss=nan]\n",
            "Epoch 00237: reducing learning rate of group 0 to 4.7830e-04.\n",
            "Epoch 238/300: 100% 66/66 [00:01<00:00, 41.50it/s, Loss=nan]\n",
            "Epoch 239/300: 100% 66/66 [00:01<00:00, 41.42it/s, Loss=nan]\n",
            "Epoch 240/300: 100% 66/66 [00:01<00:00, 41.64it/s, Loss=nan]\n",
            "Epoch 241/300: 100% 66/66 [00:01<00:00, 41.67it/s, Loss=nan]\n",
            "Epoch 242/300: 100% 66/66 [00:01<00:00, 41.66it/s, Loss=nan]\n",
            "Epoch 243/300: 100% 66/66 [00:01<00:00, 41.71it/s, Loss=nan]\n",
            "Epoch 244/300: 100% 66/66 [00:01<00:00, 41.62it/s, Loss=nan]\n",
            "Epoch 245/300: 100% 66/66 [00:01<00:00, 41.54it/s, Loss=nan]\n",
            "Epoch 246/300: 100% 66/66 [00:01<00:00, 41.13it/s, Loss=nan]\n",
            "Epoch 247/300: 100% 66/66 [00:01<00:00, 41.33it/s, Loss=nan]\n",
            "Epoch 248/300: 100% 66/66 [00:01<00:00, 37.78it/s, Loss=nan]\n",
            "Epoch 249/300: 100% 66/66 [00:01<00:00, 41.55it/s, Loss=nan]\n",
            "Epoch 250/300: 100% 66/66 [00:01<00:00, 41.55it/s, Loss=nan]\n",
            "Epoch 251/300: 100% 66/66 [00:01<00:00, 41.66it/s, Loss=nan]\n",
            "Epoch 252/300: 100% 66/66 [00:01<00:00, 41.60it/s, Loss=nan]\n",
            "Epoch 253/300: 100% 66/66 [00:01<00:00, 41.47it/s, Loss=nan]\n",
            "Epoch 254/300: 100% 66/66 [00:01<00:00, 41.17it/s, Loss=nan]\n",
            "Epoch 255/300: 100% 66/66 [00:01<00:00, 37.51it/s, Loss=nan]\n",
            "Epoch 256/300: 100% 66/66 [00:01<00:00, 41.15it/s, Loss=nan]\n",
            "Epoch 257/300: 100% 66/66 [00:01<00:00, 41.49it/s, Loss=nan]\n",
            "Epoch 258/300: 100% 66/66 [00:01<00:00, 41.55it/s, Loss=nan]\n",
            "Epoch 00258: reducing learning rate of group 0 to 4.3047e-04.\n",
            "Epoch 259/300: 100% 66/66 [00:01<00:00, 41.60it/s, Loss=nan]\n",
            "Epoch 260/300: 100% 66/66 [00:01<00:00, 41.40it/s, Loss=nan]\n",
            "Epoch 261/300: 100% 66/66 [00:01<00:00, 41.50it/s, Loss=nan]\n",
            "Epoch 262/300: 100% 66/66 [00:01<00:00, 41.58it/s, Loss=nan]\n",
            "Epoch 263/300: 100% 66/66 [00:01<00:00, 41.55it/s, Loss=nan]\n",
            "Epoch 264/300: 100% 66/66 [00:01<00:00, 41.15it/s, Loss=nan]\n",
            "Epoch 265/300: 100% 66/66 [00:01<00:00, 41.53it/s, Loss=nan]\n",
            "Epoch 266/300: 100% 66/66 [00:01<00:00, 41.55it/s, Loss=nan]\n",
            "Epoch 267/300: 100% 66/66 [00:01<00:00, 41.40it/s, Loss=nan]\n",
            "Epoch 268/300: 100% 66/66 [00:01<00:00, 37.81it/s, Loss=nan]\n",
            "Epoch 269/300: 100% 66/66 [00:01<00:00, 41.48it/s, Loss=nan]\n",
            "Epoch 270/300: 100% 66/66 [00:01<00:00, 40.85it/s, Loss=nan]\n",
            "Epoch 271/300: 100% 66/66 [00:01<00:00, 41.13it/s, Loss=nan]\n",
            "Epoch 272/300: 100% 66/66 [00:01<00:00, 41.47it/s, Loss=nan]\n",
            "Epoch 273/300: 100% 66/66 [00:01<00:00, 41.71it/s, Loss=nan]\n",
            "Epoch 274/300: 100% 66/66 [00:01<00:00, 41.49it/s, Loss=nan]\n",
            "Epoch 275/300: 100% 66/66 [00:01<00:00, 37.87it/s, Loss=nan]\n",
            "Epoch 276/300: 100% 66/66 [00:01<00:00, 41.66it/s, Loss=nan]\n",
            "Epoch 277/300: 100% 66/66 [00:01<00:00, 41.76it/s, Loss=nan]\n",
            "Epoch 278/300: 100% 66/66 [00:01<00:00, 41.39it/s, Loss=nan]\n",
            "Epoch 279/300: 100% 66/66 [00:01<00:00, 41.69it/s, Loss=nan]\n",
            "Epoch 00279: reducing learning rate of group 0 to 3.8742e-04.\n",
            "Epoch 280/300: 100% 66/66 [00:01<00:00, 41.46it/s, Loss=nan]\n",
            "Epoch 281/300: 100% 66/66 [00:01<00:00, 41.98it/s, Loss=nan]\n",
            "Epoch 282/300: 100% 66/66 [00:01<00:00, 41.75it/s, Loss=nan]\n",
            "Epoch 283/300: 100% 66/66 [00:01<00:00, 41.67it/s, Loss=nan]\n",
            "Epoch 284/300: 100% 66/66 [00:01<00:00, 41.58it/s, Loss=nan]\n",
            "Epoch 285/300: 100% 66/66 [00:01<00:00, 41.59it/s, Loss=nan]\n",
            "Epoch 286/300: 100% 66/66 [00:01<00:00, 41.74it/s, Loss=nan]\n",
            "Epoch 287/300: 100% 66/66 [00:01<00:00, 41.00it/s, Loss=nan]\n",
            "Epoch 288/300: 100% 66/66 [00:01<00:00, 37.69it/s, Loss=nan]\n",
            "Epoch 289/300: 100% 66/66 [00:01<00:00, 41.47it/s, Loss=nan]\n",
            "Epoch 290/300: 100% 66/66 [00:01<00:00, 41.55it/s, Loss=nan]\n",
            "Epoch 291/300: 100% 66/66 [00:01<00:00, 41.41it/s, Loss=nan]\n",
            "Epoch 292/300: 100% 66/66 [00:01<00:00, 41.52it/s, Loss=nan]\n",
            "Epoch 293/300: 100% 66/66 [00:01<00:00, 41.57it/s, Loss=nan]\n",
            "Epoch 294/300: 100% 66/66 [00:01<00:00, 41.70it/s, Loss=nan]\n",
            "Epoch 295/300: 100% 66/66 [00:01<00:00, 37.60it/s, Loss=nan]\n",
            "Epoch 296/300: 100% 66/66 [00:01<00:00, 41.25it/s, Loss=nan]\n",
            "Epoch 297/300: 100% 66/66 [00:01<00:00, 41.73it/s, Loss=nan]\n",
            "Epoch 298/300: 100% 66/66 [00:01<00:00, 41.50it/s, Loss=nan]\n",
            "Epoch 299/300: 100% 66/66 [00:01<00:00, 41.62it/s, Loss=nan]\n",
            "Epoch 300/300: 100% 66/66 [00:01<00:00, 41.75it/s, Loss=nan]\n",
            "Epoch 00300: reducing learning rate of group 0 to 3.4868e-04.\n",
            "Time: 16.576522827148438\n",
            "Saving sampled data to synthetic/adult/twae.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KfPFxCCrpPBW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}